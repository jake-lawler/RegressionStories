[["index.html", "Learning Stan with Regression and Other stories by Gelman, Hill &amp; Vehtari Preface", " Learning Stan with Regression and Other stories by Gelman, Hill &amp; Vehtari Jake Lawler 2022-03-31 Preface Im reading through Regression and Other Stories with two goals in mind: try to improve my conceptual understanding of regression improve my coding of base Stan. Its the second point that I want to be reflected in this notebook. I dont plan on making chapter summaries like I have with other textbooks. Instead Im going to use these notes to practice writing models in Stan. The idea is that Ill need to improve my Stan skills as the models in the book get more complicated. Because of this, my notes will not follow the structure of Regression and Other Stories, but will instead start with an introduction to Stan, and will gradually layer in complexity as I improve. I dont yet have a clear sense of the structure Im going to go for, but a rough initial guess at chapter headers might be: An Introduction to Stan Generated Quantities - Priors &amp; Predictions Multiple Predictors Model Comparison - PSIS &amp; LOO Diagnostics Logistic Regression Other GLMs Multilevel Models I have had some previous exposure to base Stan in my work-through of Statistical Rethinking, so Im not starting completely from scratch. However, I am looking forward to particularly the early portions of this book where Ill hopefully be familiar with a lot of the material and can just focus on applying it using the Stan language. Alongside the book itself, the other text Im going to be reading a lot of is the Stan User Guide and some of the other Stan documentation. That last link includes the reference manuals, but also some example case studies and tutorials which I imagine will be helpful. "],["introduction.html", "Chapter 1 An Introduction to Stan Stan Basics", " Chapter 1 An Introduction to Stan Stan Basics The first model in the book concerns the relationship between incumbent vote share and income growth in U.S. elections. Heres a plot of the data: data(hibbs) ggplot(data = hibbs, aes(x = growth, y = vote))+ geom_text(aes(label=year))+ xlab(&quot;Average recent growth in personal income (%)&quot;)+ ylab(&quot;Incumbent party&#39;s vote share&quot;) Heres the model in the chapter: m1_1 &lt;- stan_glm(vote ~ growth, data=hibbs) # saveRDS(m1_1,file=&quot;models\\\\m1_1.rds&quot;) And heres a plot of the resulting regression line: ggplot(data = hibbs, aes(x = growth, y = vote))+ geom_point()+ geom_abline(intercept = coef(m1_1)[1], slope = coef(m1_1)[2])+ xlab(&quot;Average recent growth in personal income (%)&quot;)+ ylab(&quot;Incumbent party&#39;s vote share&quot;) The stan_glm function comes from the rstanarm package, but Im trying to learn base Stan so Ill have a go at recreating it. Heres the model code. I also pair down the dataset to only the variables that will be used, which I believe is good practice. code_m1_1 &lt;- &quot;data{ int&lt;lower=0&gt; n; vector[n] vote; vector[n] growth; } parameters{ real a; real b; real&lt;lower=0&gt; sigma; } model{ vote ~ normal( a + b * growth , sigma ); }&quot; data_hibbs &lt;- hibbs %&gt;% select(vote,growth) The code has three blocks. In the data block we use \\(n\\) as a stand in for the number of observations in our data. We then specify that variables vote and growth are vectors of length \\(n\\). Our parameters are to be estimated - we have an intercept, a slope, and a noise parameter. The model block defines our model. In this case its just about the simplest possible regression model, with improper priors. Now we run the model: m1_1b &lt;- stan_model(model_name = &quot;m1_1b&quot;,model_code=code_m1_1)%&gt;% sampling(data = compose_data(data_hibbs), chains=1) #saveRDS(m1_1b,file=&quot;models\\\\m1_1b.rds&quot;) The first function here, stan_model, compiles the model and the second samples from it. The result is a stanfit object. Lets recreate the plot above using point estimates for parameters \\(a\\) and \\(b\\): draws1_1b &lt;- m1_1b %&gt;% gather_draws(a, b) %&gt;% mean_qi() ggplot(data = hibbs, aes(x = growth, y = vote))+ geom_point()+ geom_abline(intercept = draws1_1b$.value[1], slope = draws1_1b$.value[2])+ xlab(&quot;Average recent growth in personal income (%)&quot;)+ ylab(&quot;Incumbent party&#39;s vote share&quot;) This looks to be about the same line. Any differences might come from slightly different approaches to default priors in the two packages. Vectorisation in Stan The code above used a vectorised form of model specification: vote ~ normal( a + b * growth , sigma ); but its also possible to specify the unvectorised version of the same model: for (i in 1:n) { vote[i] ~ normal(a + b * growth[i], sigma); } According to the user manual, the vectorised form is faster. This section of the user manual also describes writing a regression model with more than one predictor using matrix notation. Heres the example model they use: data { int&lt;lower=0&gt; N; // number of data items int&lt;lower=0&gt; K; // number of predictors matrix[N, K] x; // predictor matrix vector[N] y; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors real&lt;lower=0&gt; sigma; // error scale } model { y ~ normal(x * beta + alpha, sigma); // likelihood } The matrix of predictors \\(x\\) multiplies the vector of coefficients \\(\\beta\\). Consider trying this way of coding models once you get to multiple regression in the book. Thats more or less the only model in the first chapter of Regression and Other Stories. so Ill leave my notes there. "],["generated.html", "Chapter 2 Generated Quantities Introduction to the Generated Quantities Block Drawing from Linear Predictor &amp; Posterior Prior Predictive Checks Adding the Log Likelihood", " Chapter 2 Generated Quantities Introduction to the Generated Quantities Block Heres the Stan model we wrote in the last chapter: data{ int&lt;lower=0&gt; n; vector[n] vote; vector[n] growth; } parameters{ real a; real b; real&lt;lower=0&gt; sigma; } model{ vote ~ normal( a + b * growth , sigma ); } In the Stan code above, we used the data, parameters, and model blocks. One block that wasnt used is the (optional) generated quantities block. It runs after the sample at each iteration, and so can be used without slowing down the sampling process too much. Lets start by seeing what variables exist in the model samples, and we can think about whats not in there that we might like to have. extract(m1_1b)%&gt;% names() ## [1] &quot;a&quot; &quot;b&quot; &quot;sigma&quot; &quot;lp__&quot; We have the three parameters we included in the model specification: the intercept \\(a\\), the slope parameter \\(b\\), and the scale parameter \\(\\sigma\\). The term lp__ is unusual. According to the reference manual, its the log posterior density (up to a constant). However the manual also describes it as deprecated, as says it should not be used. This helpful blog post describes it a little more. At each iteration, the lp__ term calculates the log likelihood of each observation given the sampled parameter values at that iteration. It then sums them, so the lp__ vector has length equal to the number of iterations. This sounds useful, so why is it deprecated and its use discouraged? Its a little over my head, but according to the above blog post, Stan scales the likelihood in such a way that the resulting log likelihood term not useful for model comparison because the scale factor changes across models. The fact that lp__ sums the log likelihoods of each observation is also a problem if we want to, say, use some importance sampling or leave-one-out approach to evaluate our models. Alright, so ignoring lp__ our model above provides us with samples for three parameters. What else might we want? Some possibilities are: Draw from the linear predictor, in the model structure above this means \\(\\mu\\) such that \\(\\mu = a + b*x\\). Draw from the posterior predictive distribution, i.e. \\(y\\) such that \\(y \\sim \\text{Normal}(\\mu,\\sigma)\\) in the model structure above. We may want the log likelihood for the reasons described above. We may want to be able to do prior predictive checks without using MCMC (conditioning on no data to return the prior), which is possible but inefficient. All of these things can be done inside the generated quantities block (I believe!), and Im going to spend the rest of this page trying to figure out how to do this. Drawing from Linear Predictor &amp; Posterior We can use the generated quantities block to make predictions given the sampled parameters and some set of predictors. Predictions on Existing Data If we want to retrodict the data, i.e. make predictions from the observed predictor values to compare them to the observed outcome values we can add the following generated quantities block to our code above: generated quantities{ vector[n] mu_pred; vector[n] y_pred; for (i in 1:n) { mu_pred[i] = a + b * growth[i]; y_pred[i] = normal_rng(mu_pred[i], sigma); } } The code above defines two new quantities, mu_pred and y_pred. mu_pred is the linear predictor i.e \\(\\mu\\) in the model structure below: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu_i, sigma) \\\\ mu_i &amp;= \\alpha + \\beta x_i \\end{aligned} \\] And y_pred is the posterior prediction - it includes the uncertainty induced by the \\(\\sigma\\) term. In the code above, the predictions are performed using the sampled parameter values and the existing predictor (in this case growth) values. In a simple model like this one, its maybe overkill to define both of these quantities separately in our model. Maybe we only care about the posterior predictions. However, when we get to generalised linear models where the linear predictor is transformed before being fed into whatever distribution were using, it will be more useful to have both. Adding the above generated quantities block to our existing model code, compiling it, and sampling makes our new quantities available to us: extract(m_gen_1)%&gt;% names() ## [1] &quot;a&quot; &quot;b&quot; &quot;sigma&quot; &quot;mu_pred&quot; &quot;y_pred&quot; &quot;lp__&quot; What structure do our new quantities have? The get_variables function from tidybayes tells us what variables are available to us, and this gives us a hint: get_variables(m_gen_1) ## [1] &quot;a&quot; &quot;b&quot; &quot;sigma&quot; &quot;mu_pred[1]&quot; ## [5] &quot;mu_pred[2]&quot; &quot;mu_pred[3]&quot; &quot;mu_pred[4]&quot; &quot;mu_pred[5]&quot; ## [9] &quot;mu_pred[6]&quot; &quot;mu_pred[7]&quot; &quot;mu_pred[8]&quot; &quot;mu_pred[9]&quot; ## [13] &quot;mu_pred[10]&quot; &quot;mu_pred[11]&quot; &quot;mu_pred[12]&quot; &quot;mu_pred[13]&quot; ## [17] &quot;mu_pred[14]&quot; &quot;mu_pred[15]&quot; &quot;mu_pred[16]&quot; &quot;y_pred[1]&quot; ## [21] &quot;y_pred[2]&quot; &quot;y_pred[3]&quot; &quot;y_pred[4]&quot; &quot;y_pred[5]&quot; ## [25] &quot;y_pred[6]&quot; &quot;y_pred[7]&quot; &quot;y_pred[8]&quot; &quot;y_pred[9]&quot; ## [29] &quot;y_pred[10]&quot; &quot;y_pred[11]&quot; &quot;y_pred[12]&quot; &quot;y_pred[13]&quot; ## [33] &quot;y_pred[14]&quot; &quot;y_pred[15]&quot; &quot;y_pred[16]&quot; &quot;lp__&quot; ## [37] &quot;accept_stat__&quot; &quot;stepsize__&quot; &quot;treedepth__&quot; &quot;n_leapfrog__&quot; ## [41] &quot;divergent__&quot; &quot;energy__&quot; There are 16 variables each for mu_pred and y_pred, one for each predictor value. Each of these variables has been sampled 1,000 times, so we get a full posterior distribution for each and not just a point prediction. We can extract these draws using tidybayes functions, for example: m_gen_1%&gt;% spread_draws(mu_pred[i], y_pred[i])%&gt;% head() ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## Please use `gather()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## # A tibble: 6 x 6 ## # Groups: i [1] ## i mu_pred .chain .iteration .draw y_pred ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 52.2 1 1 1 50.8 ## 2 1 55.1 1 2 2 56.3 ## 3 1 55.6 1 3 3 58.4 ## 4 1 51.4 1 4 4 49.2 ## 5 1 54.8 1 5 5 52.2 ## 6 1 54.2 1 6 6 55.3 Lets plot them over the existing data: # We extract the draws from the model draws_gen_1 &lt;- m_gen_1%&gt;% gather_draws(mu_pred[i], y_pred[i])%&gt;% mean_qi(.width = 0.95)%&gt;% # specify the 95% posterior interval mutate(growth = data_hibbs$growth[i]) # we need to add the predictor values to position on the x-axis ggplot(data = draws_gen_1, aes(x = growth, y = .value))+ geom_pointinterval(aes(ymin = .lower, ymax = .upper, colour = .variable), # plot the posterior intervals position = position_dodge(width = -0.1))+ geom_point(data = hibbs, aes(y = vote))+ # plot the initial date xlab(&quot;Average recent growth in personal income (%)&quot;)+ ylab(&quot;Incumbent party&#39;s vote share&quot;) This probably isnt the clearest way to present these predictions, but weve proven the concept: we can extract and work with the the posterior distributions for \\(\\mu\\) and \\(y\\). Predictions on New Data Prior Predictive Checks https://stackoverflow.com/questions/57703920/sampling-from-prior-without-running-a-separate-model Adding the Log Likelihood https://vasishth.github.io/bayescogsci/book/cross-validation-in-stan.html#psis-loo-cv-in-stan Further Reading {-} To make predictions on new data: User Manual page 43 Blog post on lp__: https://www.jax.org/news-and-insights/jax-blog/2015/october/lp-in-stan-output "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
